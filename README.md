# Generating Piano Sounds Using DiffusionLM

**Collaborators:** [Chuhan Li](https://github.com/LeeChuh) and [Ruiqi Liu](https://github.com/RRQLiu) 

**Note:** This is our semester-long research project for Boston University's Fall 2022 course - `CAS CS 523: Deep Learning` taught by Professor Iddo Drori. 

This repository contains the implementation of our project. Inspired by the recent adaptation of language models to generate audio and music, we explored an innovative way to adapt Diffusion Language Models (`Diffusion-LM`) to produce piano sounds. Capitalizing on the characteristic features of diffusion language models to generate structured sequential data, we used `Diffusion-LM` to produce music by placing it in the discrete music domain. Through the use of `MIDI` representation of music, we were effectively able to train our models and generate music. In our project, we explored both `BERT` and `Electra-BERT` as base networks for the generative model.

## Contents:
- [Introduction](#Introduction)
- [Data](#Data)
- [Methodology](#Methodology)
- [Results](#Results)
- [Sample Output](#Sample-Output)

### Introduction:




### Sample-Output

https://github.com/LeeChuh/DiffusionLM_MIDI_Generator/assets/60937990/a5be22c7-ba96-4c07-8598-240d3c29eb75

https://github.com/LeeChuh/DiffusionLM_MIDI_Generator/assets/60937990/3962d9c7-6d59-4e00-b1a9-ea450ac945bf

https://github.com/LeeChuh/DiffusionLM_MIDI_Generator/assets/60937990/0e867d77-fdc4-4359-9130-b289966b255f

https://github.com/LeeChuh/DiffusionLM_MIDI_Generator/assets/60937990/4e9a4315-e4d1-4361-af4f-b3fcc62989c0
